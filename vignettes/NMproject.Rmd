---
title: "NMproject"
author: "Tarj Sahota"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{NMproject}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Why NMproject?

## History

NMproject's initial outing (the "alpha" interface) was developed in AZ and internally had 83% voluntary user retention ("user" = someone with 5+ separate model development workflows) across 100s of analyses in all TAs.  It was a prototype R package produced to demonstrate the concept of industrialising pharmacometric analyses via script based model development.  Key benefits included:

1. script-based model development workflows being used as the primary location to record modeller thoughts/notes/decisions alongside reproducible, re-executable code

2. a code library to encourage standardisation and sharing of best coding practices

3. compatibility with PsN and Pirana

4. installable in a wide variety of infrastructures in multiple organisations, from standalone windows installations to large unix based clusters

## New interface

This is the new "beta" interface.  A completely redesigned syntax that address several shortcoming of the previous syntax and expands functionality.  Why the redesign?

-	(*New*)End-to-end model development workflows/notebooks for people who want 100% control over their model files.  The new interface is the only NONMEM interface (known to the author) that can record and replay manual edits to files (seemlessly) for 100% flexibility without breaking the end-to-end reproducibility.  

-	(*New*)R Functions to automatically fill $INPUT, $DATA, $THETA,… elements as well as various other routine model file manipulations

- (*New*)Import code library templates (via a shiny interface) and get to a working NONMEM model quickly and entirely within R.  See demo.

-	(*New*)NMproject is the only R package (known to the author) with a vectorized model object allowing groups of models to be operated on using the same syntax as a single model.

-	(*New*)Custom NMproject implementations of bootstrap, cross-validation, PPCs, stepwise covariate selection, and simulation-re-estimation. All controllable on granular level using vectorized model objects.

-	Diagnostics and VPCs using your favourite packages (e.g., xpose, vpc, …)

-	(*New*)RStudio addins to streamline user experience.

-	Monitor runs via shiny app including interactive OFV vs iteration plots for convergence assessment.

-	Optional customizable analysis directory structure for consistent code organisation

- (*New*)R markdown friendly. Pipe friendly

NMproject has been used to conduct exploratory analyses and submission work.  Model based power calculations were performed with NMproject in collaboration with biostatistics to demonstrate feasibility, design (sample size and dose), and to plan interim decision points of a recently published oncology dose finding study using pharmacometric endpoints: [video presentation](https://youtu.be/QVw28Im3Zz0)

## Prerequisites

NMproject targetted towards mid to upper level R users. If you know what a pipe is, what knitr is, you should be good.  NMproject does not hide NONMEM from the user so users should be familiar with NONMEM coding.  For advanced functionality, knowledge of dplyr will help to create complex workflows.


```{r setup, include=FALSE}
library(knitr)
library(NMproject)
library(dplyr)
extdata_path <- system.file("extdata", package = "NMproject")

testfilesloc <- file.path(extdata_path, "theopp")
zip_file <- file.path(extdata_path, "theopp.zip")

files_to_unzip <- unzip(zip_file, list = TRUE)

unzip(zip_file)

dir(all.files = TRUE)
extracted_folders <- dir("theopp", full.names = TRUE)

for(extracted_folder in extracted_folders){
  file.copy(extracted_folder, ".", recursive = TRUE)
}

file.rename("cache", ".cache")
dir(all.files = TRUE)

copied_folders <- basename(gsub("cache", ".cache", extracted_folders))

unlink("theopp", recursive = TRUE)

orig_dir <- getwd()

```

# Demo: Learning by doing

The easiest way to familiarise your with NMproject is to run through the demo.  Here's a short YouTube video on running the demo

<iframe width="560" height="315" src="https://www.youtube.com/embed/nAkcEFz0RLg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Run the following in the new R session:

```{r eval = FALSE}
library(NMproject)
setup_nm_demo()
```

This will population the `Scripts` directory with scripts, and deposit the Theopp dataset into the `SourceData` subdirectory.

The scripts are numbered `s01.....Rmd`, `s02....Rmd` etc., and are designed to be read and run in order.  The scripts can also be used as template for your own model development

# Code and data preparation

## Creating Analysis Project 

It is recommended to work in structured.  Create a new analysis project via the RStudio menu items: `FILE` -> `New Project` -> `New Directory` -> `New NMproject`.  The underlying function being used to create this analysis project is `nm_create_analysis_project()`.  See documentation for detailed information including how to modify the structure to suit your preferences.

Follow through the instructions, you'll be asked for a location, a name and whether you want to use `renv` to manage project library directories.  See `renv` documentation for more information.

<iframe width="560" height="315" src="https://www.youtube.com/embed/nAkcEFz0RLg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Once created you'll see a clean analysis directory with empty subfolders.

Default the subdirectories for model development (these can be modified - see `nm_create_analysis_project()` documentation):

- SourceData : intended for unmodified source datasets entering the analysis project

- DerivedData : intended for cleaned and processed NONMEM ready datasets

- Scripts : intended for all R scripts

- Models : intended for all NONMEM modelling

- Results : intended as default location for run diagnostics, plots and tables

## Creating new R markdowns

There is nothing mandating the use of R markdown in NMproject.  You can use scripts.  However R markdown documents produce nice sharable model development logs which provide a readable description of what steps were performed and in what order.

It is advisable to always start from a template.  Templates can be accessed in `File` -> `New File` -> `Rmarkdown...` -> `From Template`.  Two to get started with are `NMproject generic` which we'll use as a generic template for data processing and cleaning and `model development` for our model development notebook.

## Data cleaning

Your first script in performing an analysis will probably be a dataset assembly or dataset cleaning.  It's easiest to start with the generic `NMproject generic` R markdown template.

The intent of the `SourceData` directory is to serve as an entry point of raw or externally produced dataset into your analysis directory.

It is *best practice* to use relative paths in your code where possible and to avoid using `setwd()` commands to change the working directory.  E.g. to read a data set in the `SourceData` directory, use `read.csv("SourceData/data.csv")`.

The `write_derived_data()` function can be used to save the NONMEM ready dataset into the `DerivedData` directory:

```{r eval = FALSE}

d %>% write_derived_data("DerivedData/data.csv")

```

See the demo for how/why to create bootstrapped datasets before beginning model development.

## Code Library

It's always best to start with NONMEM template too.  Search the code library and bring into staging area of analysis project using the `code_library` addin:

One of the templates is `Models/ADVAN2.mod`.  Select it, `preview` it and then `stage` by clicking the buttons and then closing the app.

<iframe width="560" height="315" src="https://www.youtube.com/embed/nTixZB9tfgk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

The model will be in the staging area: `staging/Models/ADVAN2.mod`, not `Models/ADVAN2.mod`.  Files in the staging area are not to be modified. It's similar to the `SourceData` directory in that sense.  This is to ensure that we can make changes to `ADVAN2.mod` in the code library without breaking old analyses that depended on it.

# Model development

`File` -> `New File` -> `New Rmarkdown...` -> `From Template` -> `Model development` will get you started with a model development log file.

## NM objects

The core nm object is created via the `new_nm()`.  Subsequent child objects are created via the `child()` function. All interactions with NONMEM occur through the this object.  It contains metadata about the NONMEM run and contain the contents of what will writtent as the control file (also called model file).

NONMEM control files are only written to the `Models` sub-directory just before running NONMEM via the `run_nm()` command

Let's create our first NONMEM object with `new_nm()`.  This will be a parent run to all other runs and thus requires more set up than other runs. Subsequent child objects created with `child()` will inherits characteristics of the parent.

Three arguments are required to create the parent, a run identifier, `run_id`, a control file it's based on, `based_on`, and a (relative) dataset path `data_path`:

```{r include=FALSE}

ls_code_library("Models/ADVAN2.mod") %>%
  stage()

```


```{r}

m1 <- new_nm(run_id = "m1",
             based_on = "staging/Models/ADVAN2.mod",
             data_path = "DerivedData/data.csv")

m1

```

Display the object by typing `m1` in the console.  Notice that the `run_in` field is point to `Models`.  This can be changed by piping the function of the same name e.g. 

```{r}

m1 <- new_nm(run_id = "m1",
             based_on = "staging/Models/ADVAN2.mod",
             data_path = "DerivedData/data.csv") %>%
  run_in("NONMEM/base_model")

```

will run all models in a subdirectory of a subdirectory `NONMEM/base_model/` instead (the $DATA (relative) path to the dataset will be automatically be updated to reflect the new location of the control file).  Any field can by modified using a similar heuristic.

Piping is encouraged because it enables you to read the sequence of events in the order that they occur.  Here we just have a single pipe, but you'll see that we will frequently pipe longer chains of commands together.  Each step is applying a transformation to the core nm object.  The chain can be partially run in the console to see what each transformation is doing.  The rest of this document is full of pipes so if you are unfamiliar with pipes, please consult the `magrittr` documentation.

To extract a field (rather than set a field) use the same function without additional arguments:

```{r}

run_in(m1)

```

For now though, we'll remove the last piped command and stay with the default `run_in` location.  

*NOTE:* the field `ctl_name` refers to the name of the control that will be created.  This will only be created when it the model is run with the `run_nm()` function (described later).  For now, the control file contents reside inside the object.  To view these you can use `show model/ctl file` addin , or `text(m1)`.

A few automatic edits from the staged control file and a compact representation of these changes can be shown by highlighting the above code and selecting the `nm_diff` addin which show what has been changed.

<iframe width="560" height="315" src="https://www.youtube.com/embed/mYUaKrj0-YE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Learning how to read to diffs will be an important skill in NMproject you will pick up over time.  Notice how the `$DATA` has been updated to refer to the new location.

The default `cmd()` field of the object is `execute {ctl_name} -dir={run_dir}`.  The braces are referred to as `glue` fields using the `glue` package.  These refer to field names of the object that will be substituted in.  For completeness on the next step we will explicitly set this to ensure our model development is easy to read.

```{r eval=FALSE}

m1 <- new_nm(run_id = "m1",
             based_on = "staging/Models/ADVAN2.mod",
             data_path = "DerivedData/data.csv") %>%
  cmd("execute {ctl_name} -dir={run_dir}")

```

The final steps to gets the NONMEM model ready is to fill in the remaining blanks in the template.  They are the `$INPUT` and `$THETA`, `$OMEGA`.  For this we will use the `fill_input()` and `init_theta()` and `init_omega()`.

```{r eval=FALSE}

m1 <- new_nm(run_id = "m1",
             based_on = "staging/Models/ADVAN2.mod",
             data_path = "DerivedData/data.csv") %>%
  cmd("execute {ctl_name} -dir={run_dir}") %>%
  fill_input() %>%
  init_theta(init = c(-2, 0.5, 1)) %>%
  init_sigma(init = c(0.1, 0.1))

```

## executing NONMEM

Thus far, we have not executed NONMEM nor saved the control file to the file system.  To execute, we simply pipe into the `run_nm()` function which will often form the last step of the chain and then run the command.

```{r eval=FALSE}

m1 <- new_nm(run_id = "m1",
             based_on = "staging/Models/ADVAN2.mod",
             data_path = "DerivedData/data.csv") %>%
  cmd("execute {ctl_name} -dir={run_dir}") %>%
  fill_input() %>%
  init_theta(init = c(-2, 0.5, 1)) %>%
  init_sigma(init = c(0.1, 0.1)) %>%
  run_nm()

```

## Fast NMTRAN checks

PLACEHOLDER: YouTube video showing nm tran addin in action.

In a cluster environment it is especially important to be able to diagnose data and control file issues before sending jobs off to the grid.  Otherwise, you often need to wait minutes for errors that can be spotted immediately.  It's therefore highly recommended to make use of the `nm_tran` addin.  This will only run NMTRAN checks to find control file syntax errors and dataset errors.  It will not run nonmem itself.  To use this addin, highlight the above code and select the `nm_tran` addin near the top of the RStudio GUI.

## Manual edits

Often there will not be the functions to do the control file manipulation you want.  Although it is preferable to stick to automatic control file manipulation functions, you can do full tracked `manual_edit`s via the addin menu.  Again, just highlight the code, click the addin and follow the instructions:

<iframe width="560" height="315" src="https://www.youtube.com/embed/HyCHeAAiwy4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Shiny run monitor

To view all runs in the workspace and track progress:

```{r eval=FALSE}
shiny_nm()
```

## Diagnostics

Use markdown templates to create a customised set of diagnostics to reuse on multiple models.  In the demo an example is shown in `Scripts/basic_gof.Rmd`, but ideally you'll create your own customised version with everything you need to evaluate your model. To create a rmarkdown diagnostic template go to `FILE` -> `New File` -> `R markdown` -> `From Template` then select from one of the following:

- `model diagnostic`

- `VPC diagnostic`

- `PPC diagnostic`

- `bootstrap results`

*PLACEHOLDER: short youtube video of opening, customizing, saving and running a template.*

A template will appear in the script window for you to customise.  Instructions are the at the top.  Save the file (e.g. as `Scripts/basic_gof.Rmd`) and run in your log script with:

```{r eval=FALSE}
m1 <- m1 %>% nm_render("Scripts/basic_gof.Rmd")
```

It will create the output in `Results` (or `results_dir(m1)`)

## Automatic edits

NMproject contains several functions for automatic control file edits.  We have already seen `fill_input()` and `init_theta()` etc.  There are higher order functions which make multiple changes to your control stream, one of which is the `subroutine()`.  If we already have a parent run `m1` using `ADVAN2 TRANS1`, we can create a `child()` run that uses `TRANS2` using:

```{r include=FALSE}

m1 <- dir(".cache", full.names = TRUE) %>%
  subset(grepl("m1$",.)) %>%
  {as_nm_list(readRDS(.)$object)}

m2 <- dir(".cache", full.names = TRUE) %>%
  subset(grepl("m2$",.)) %>%
  {as_nm_list(readRDS(.)$object)}

m2WT <- dir(".cache", full.names = TRUE) %>%
  subset(grepl("m2WT$",.)) %>%
  {as_nm_list(readRDS(.)$object)}

m1orig <- m1

## need to skip overwrite because new user will mean directory gets overwriten
overwrite_behaviour("skip") 

```

```{r eval=FALSE}

m2 <- m1 %>% child() %>%
  subroutine(trans = 2) %>% 
  run_nm()

```

View exactly what's been changed by highlighting the above code in RStudio and clicking the `nm_diff` addin to see what's been changed before running.  Here, changes will be in the `$SUB`, `$PK` and `$THETA`.

To add a covariate using PsN coding conventions use `add_cov()`:

```{r eval=FALSE}

m2WT <- m2 %>% child(run_id = "m2WT") %>%
  add_cov(param = "CL", cov = "WT", state = "linear") %>%
  run_nm()

```

Apply a fully editable goodness of fit R markdown template to both runs `m2` and `m3`:

```{r eval=FALSE}

c(m1, m2) %>% nm_render("Scripts/basic_gof.Rmd")
## will produce two run specific html reports in the "Results" directory for run evaluation

```

Note: `c(m1, m3)` is a vector object of 2 NONMEM runs.  We'll touch more on this later.

Evaluate and compare runs on the fly with the following commands

```{r}
rr(c(m2, m2WT))
plot_iter(m2, skip = 10) ## skip first 10 interations 
covariance_plot(m2)
```

However these are better placed inside templates to enable rapid and consistent re-use.

## Simulation based diagnostics

Create simulation based diagnostics first by running a simulation using `update_parameters()` and `convert_to_simulation()`.  Then use the `ppc diagnostics` and `vpc diagnostics` R markdown templates to generate customisable vpc and ppc diagnostics reports.
  
```{r eval=FALSE}
m2s <- m2 %>% child(run_id = "m2s") %>%
              update_parameters(m2) %>%
              convert_to_simulation(subpr = 50) %>%
              run_nm()

m2s %>% nm_render("Scripts/basic_vpc.Rmd")
m2s %>% nm_render("Scripts/basic_ppc.Rmd")

```

Don't forget to comment your code with your decision making.

## Clean up runs

```{r eval=FALSE}

m1 %>% ls_tempfiles() ## will list all temporary files associate with run m1

## remove all temporary files associated with m1
m1 %>% ls_tempfiles() %>% unlink()

## Not specifying an argument will list temporary files of all runs
ls_tempfiles()

```

## Running NONMEM with parallel processing

Two fields `parafile` and `cores` when combined with a suitable `cmd` can enable runs to be executed in parallel using PsN.

```{r eval=FALSE}
m1 <- new_nm(run_id = "m1",
             based_on = "staging/Models/ADVAN2.mod",
             data_path = "DerivedData/data.csv") %>%
  cmd("execute {ctl_name} -parafile={parafile} -dir={run_dir} -nodes={cores}") %>%
  parafile("/opt/NONMEM/nm750/run/mpilinux8.pnm") %>%
  cores(4) %>%
  run_nm()

```

```{r include = FALSE}
m1 <- m1 %>%
  cmd("execute {ctl_name} -parafile={parafile} -dir={run_dir} -nodes={cores}") %>%
  parafile("/opt/NONMEM/nm750/run/mpilinux8.pnm") %>%
  cores(4)
```

## Bootstraps

The package `rsample` can be used to create bootstrap datasets in your initial data manipulation scripts.  The following is an example bootstrap dataset being prepared with stratification on `SEX` and bodyweight `WTC` categorised in two categories  

```{r eval = FALSE}

d <- d %>%
  mutate(WT_C = cut(WT, breaks = 2, labels = FALSE),
         STRATA = paste(SEX, WT_C, sep = "_"))

d_id <- d %>% distinct(ID, STRATA)

set.seed(123)

## create large set of resamples (to enable simulation to grow without ruining seed)
bootsplits <- rsample::bootstraps(d_id, 100, strata = "STRATA")

dir.create("DerivedData", showWarnings = FALSE)
bootsplits %>% saveRDS("DerivedData/bootsplit_data.csv.RData")

```

In a model development script, the following, performs a 100 sample bootstrap of model `m1`

```{r eval=FALSE}

m1_boot <- m1 %>% make_boot_datasets(samples = 100, overwrite = TRUE)

m1_boot$m %>% run_nm()

## the following bootstrap template will wait for results to complete
m1_boot$m %>% nm_list_render("Scripts/basic_boot.Rmd")

```

Results can be viewed in `Results/basic_boot.m1.html`.

## Stepwise covariate selection

Covariate analysis in NMproject is part way between PsN's SCM and completely manual process to allow you to apply your full model selection criteria to each forward and backward elimination step rather than being constrained to log-likelihood ratio tests.  The syntax of how covariates are included is the same as PsN's SCM routine - See PsN documentation for more information there.

Relationships to tests are defined using the `test_relations()` function.  Below we test `BWT` and `AGE` on all parameters in both `linear` and `power` fashion and all parameters are test with the categorical covariate `SEX`.

```{r eval = FALSE}

dtest <- test_relations(param = c("KA", "K", "V"),
                        cov = c("BWT", "AGE"),
                        state = c("linear", "power"),
                        continuous = TRUE)
dtest <- dtest %>%
             test_relations(param = c("KA", "K", "V"),
                            cov = "SEX",
                            state = "linear",
                            continuous = FALSE)

```

The workflow for forward (and backward) steps then becomes

1. create a `tibble` of runs for the next step with `covariate_step_tibble()`
2. run all with `run_nm()`
3. collect results with `bind_covariate_results()`
4. evaluate top performing runs
5. Either select a run to take to the next step (go back to step 1) OR stop.

Here are steps 1-3 in action:

```{r eval = FALSE}

## create tibble of covariate step with model objects as column m
dsm1 <- m1 %>% covariate_step_tibble(
  run_id = "m1_f1",
  dtest = dtest,
  direction = "forward"
)

## run all models greedily
dsm1$m <- dsm1$m %>% run_nm()

## extract results and put into tibble
dsm1 <- dsm1 %>% bind_covariate_results()


```

Here are steps 4-5: evaluate and select

```{r eval = FALSE}

## sort by BIC (for example) and view
dsm1 <- dsm1 %>% arrange(BIC)
dsm1

## check condition number, covariance,...

## run diagnostic reports on the top three
dsm1$m[1:3] %>% nm_render("Scripts/basic_gof.Rmd")

## In this case we selec the first: dsm1$m[1]
m1_f1 <- dsm1$m[1]   ## select most signifcant BIC

```

Do next forward step

```{r eval = FALSE}

dsm2 <- m1_f1 %>% covariate_step_tibble(
  run_id = "m1_f2",
  dtest = dtest,
  direction = "forward"
)

```

continue for as many steps as needed

```{r include=FALSE}
m1 <- dir(".cache", full.names = TRUE) %>%
  subset(grepl("m1$",.)) %>%
  {as_nm_list(readRDS(.)$object)}
```

## Setting initial estimates

The functions `init_theta`, `init_omega` and `init_sigma` can be used to get and set initial estimation, parameter bounds, names, units, etc.  The work much like the functions used to access fields of the nm object

```{r}

## return a tibble version of $OMEGA with init_omega()
io <- m1 %>% init_omega()
io 

```

Note that `io` is a list of a data.frame.  This makes manipulation a bit more difficult, but referring to `io[[1]]` instead `io` will allows you to manipulate io with R's standard data.frame manipulation functions.  There are also built in functions like `block` and `unblock` (to create $OMEGA BLOCKS for correlated random effects).

```{r}

io <- io %>% block(c(2,3))  ## make block out ETA 2 and 3

## put modified io wit
m1 <- m1 %>% init_omega(io)
m1 %>% dollar("OMEGA")

```

```{r}

## for demo purposes we'll reverse the process with unblock()
io <- m1 %>% init_omega()
io <- io %>% unblock(c(2,3))
m1 <- m1 %>% init_omega(io)
m1 %>% dollar("OMEGA")

```

## Perturbing initial parameters

To modify initial estimates, we'll use the `mutate` like behaviour of `init_*` functions.  We will modify the `init` by referencing itself.  We'll modified all our fixed effects (log transformed) by 30%

```{r include=FALSE}
m1 <- dir(".cache", full.names = TRUE) %>%
  subset(grepl("m1$",.)) %>%
  {as_nm_list(readRDS(.)$object)}
```

```{r eval=FALSE}

m1 <- m1 %>% init_theta(init = rnorm(init, mean = init, sd = 0.3))

```

View with `m1 %>% dollar("THETA")`.

# Advanced topics

## Vectorization

The basic nm object is fully vectorized.  To see this compare the following two outputs

```{r}

m1
c(m1, m2)

```

Both are nm objects with length 1 and 2, respectively.  We used the `c(m1, m2)` above when we wanted to use `nm_render()` on both runs.  We didn't need to write any loops or special code to handle this because nm objects and the functions that operate on them have been designed with parallelisation in mind.  This is because in pharmacometrics we are often dealing with multiple models, perhaps moreso than other statistical modelling disciplines.

The vectorized nature of the nm object is particularly powerful when used in conjuction with `dplyr`.  To demonstrate, lets repeat the the previous initial perturbation exercise, but create 5 runs each with their own perturbed initial estimates.

The basic idea is to contruct a `tibble` (if you are unfamiliar with `tibble`s, they are a tidyverse expansion of the concept of a `data.frame`), and then use a `dplyr::mutate` statement to construct a column vector of nm objects.  Below the column vector is produced by starting with the parent object `m1` and then supplying a vector rather than a scalar to the `child()` function.  Since the `rep` column is length 5, this will make the nm_object length 5.

```{r eval = FALSE}

dp <- tibble(rep = 1:5)
dp$m <- m1 %>% ## start with parent - m1 is length 1
  child(run_id = dp$rep) %>% ## now object is length = length(dp$rep) = 5
  init_theta(init = rnorm(init, mean = init, sd = 0.3)) %>% ## vectorized operation
  run_nm() ## run all 5 runs

```

View `dp` to see a tibble with two columsn `rep` and `m`.  View the new `$THETA` statements with `dp$m %>% dollar("THETA")`.

An alternative but equivalent way of writing the above is to use dplyrs `mutate` statement to create the `m` column

```{r eval = FALSE}

dp <- tibble(rep = 1:5) %>%
  mutate(
    m = m1 %>% ## start with parent - m1 is length 1
      child(run_id = rep) %>% ## now object is length = length(dp$rep) = 5
      init_theta(init = rnorm(init, mean = init, sd = 0.3))
  )

## the m column of dp is a nm object vector, display all $THETAs like this
dp$m %>% dollar("THETA")

## run all runs in dp$m with the normal syntax 
dp$m %>% run_nm()

```

The reset of the code in this vignette will use the dplyr layout.

Note that here the functions `init_theta`, `dollar` and `run_nm` were all operating in a vectorized fashion

## Automating model checking

Lets expand the earlier subroutine example to build a vector of runs that test multiple different `ADVAN`s and `TRANS` at the same time.

We'll use a similar structure to the previous section using `.available_advans` to list all available advans, `filter` to isolate specific advans, and `mutate` and `subroutine()` to perform the control file manipulation function.

Then we'll display all the `$PK` subroutines to view the changes.


```{r}

.available_advans ## display available advans

dt <- .available_advans %>%
  ## filter only for oral dosing advans
  filter(oral %in% TRUE) %>%
  ## mutate state create a column vector m of nm objects
  ## first step is to create children runs from the parent object m1
  ## this is done by supplying a vector of run_ids to the child() function
  mutate(m = m1 %>% ## start with parent m1 again
           child(run_id = label) %>% ## create multiple children using label column
           subroutine(advan = advan, trans = trans) ## set subroutine using advan and trans columns
         )

## view the $PK blocks of each
dt$m %>% dollar("PK")

```

Let's run these, summarise the results, and generate goodness of fit diagnostics for the ones that gave somewhat reasonable outputs

```{r eval = FALSE}

## run them all and wait for them to finish
dt$m %>% run_nm() %>% wait_finish()

## summarise all runs in a table
summary_wide(dt$m)

## plot goodness of fits for all runs with ofv < 120
dt$m %>% 
  subset(ofv(.) < 120) %>%  ## subsetting is a powerful way of isolating functions to particular model objects
  nm_render("Scripts/basic_gof.Rmd")

```

## Parallel efficiency test

Often you'll want to know the right level of parallelisation to run your model to maximise speed without wasting too many resources.  The following creates multiple runes with different levels of parallelisation. We'll just test it across 1, 3, 10, and 30 cores, but this can be any vector.

```{r}

dc <- tibble(cores = c(1, 3, 10, 30)) %>% 
  mutate(m = m1 %>% ## start with parent m1 again
           child(run_id = cores) %>%  ## use "cores" as run_id (object is now 3 models)
           run_in("Models/m1_coretest") %>% ## run them all in m1_coretest
           cmd("execute {ctl_name} -parafile={parafile} -dir={run_dir} -nodes={cores}") %>% ## parallelized execute
           parafile("/opt/NONMEM/nm75/run/mpilinux8.pnm") %>%
           cores(cores) ## and finally set the cores to the "cores" vector
         )

dc$m %>% cmd()

```

```{r eval = FALSE}

## run them all and wait for them to finish
dc$m %>% run_nm() %>% wait_finish()

## extract job statistics and plot cores vs Rtime or Ttime to get plots of run
## time and total time vs number of CPUs

dc$m %>%
  job_stats() %>%
  ggplot(aes(x = cores, y = Rtime)) + theme_bw() +
  geom_point()
  
```

## Simulation re-estimation

There will soon a be a simple wrapper for the code below, as with the bootstrap and step wise covariate functionaliy above, but for now the code below is a good example of how the flexibility of the vectorized object can be used to create complex workflows whilst still providing granular control of runs.

It's good advice to start with 1 or 2 replicates and scale up only when you've confirmed your code is working (here we're just using 3 for demo purposes).  You will not waste time because `run_nm()` will skip over runs that have already completed. 

```{r eval=FALSE}

n_sims <- 3  ## start small, scale up later

dsr <- tibble(sim = 1:n_sims) %>%
  mutate(
    msim = m1 %>%              ## start with single parent run, m1, an nm object of length 1
      update_parameters() %>%  ## update inits to finals, here nm object is still length 1
      child(run_id = sim, parent = m1) %>%  ## at this point it becomes length n_sims
      run_in("Models/m1_simest") %>% ## this applies the run_in modification to all n_sims runs
      convert_to_simulation(subpr = 1, seed = sim) ## converts all to simulation
  )

## run, wait, read results and then write to run_dir paths of simulations
dsr$msim %>% run_nm() %>% 
  wait_finish() %>%
  output_table(only_append = "DV_OUT") %>%
  write_derived_data(file.path(run_dir_path(dsr$msim), "simdata.csv"))

## Now create mest column
dsr <- dsr %>%
  mutate(
    mest = m1 %>% child(run_id = sim) %>%  ## estimations derived from m1
      run_in("Models/m1_simest/est") %>%   ## run in a new subdirectory
      data_path(file.path(run_dir_path(msim), "simdata.csv")) %>%    ## set new data_paths
      ## refill $INPUT. Rename DV to be DV_OUT column. Run nm_diff() command below to see
      ## what has changed
      fill_input(rename = list("DV_OBS" = "DV", "DV" = "DV_OUT"))
  )

# nm_diff(dsr$mest[1])

dsr$mest %>% run_nm() %>% 
  wait_finish() %>%
  summary_wide(parameters = "all")

```



# FAQ

### + I have a pre-prepared model and dataset. What's the fastest way to make an nm object and make use of the objects functionality for scripting, post processing etc.

Ensure the dataset and model file are somewhere in the project.

```{r eval = FALSE}

m1 <- new_nm(run_id = "m1", 
             based_on = "path/to/control/file") %>%
  run_nm()

```

### + After having closed my session how do I recreate my workspace and pick up where I left off

Just re-run all the code again.  NONMEM runs that have already been run will not be run again and will instead be retrieved from the cache.

### + In my organisation we have a custom "execute" command/wrapper we can't use the default `execute {ctl_name} -dir={run_dir}`, how can we change this?

Two options:

1) set `cmd` once for your parent object and then make all other objects child objects from the parent.

2) set the `nm.cmd_default` option for the analysis project for your organisation.  E.g. organisations using docker (as described in below FAQ question) require the execute command to be much longer: `"docker run --rm --user=$(id -u):$(id -g) -v $(pwd):/data -w /data humanpredictions/psn execute {ctl_name} -dir={run_dir}"`.

`options(nm.cmd_default = "docker run --rm --user=$(id -u):$(id -g) -v $(pwd):/data -w /data humanpredictions/psn execute {ctl_name} -dir={run_dir}")`

Setting this will ensure all runs produced in the R session will run using the above command.  To see how to set an option for all your R sessions see subsequent question

### + How do I set a NMproject configuration option permanently so I don't have to set it for all my future R sessions?

See ?.Rprofile.  In a multiple user environment it's best practice to avoid user specific configurations as your code may run different from user to user.  There are two options, set the option for the project or site wide for all users.

To set the option for anyone using the project, include the `options` command (e.g. `options(nm.cmd_default = ....)`) in `.Rprofile`.

To set the option site wide include the `options` command in `$R_HOME/etc/Rprofile.site` (requires administration privileges).

Also consider creating your own organisation specific option/path setting package.  The `options` statement then just needs to be in a R script in the packages `R` directory.  Loading this package will then set the option.  So be sure to load it (before or after) each time you're using `NMproject`.  The advantage of a package is you can also bundle other functions in there for them to use.

### + How can I contribute to the PMX code library?

The repository `tsahota/PMXcodelibrary` may be discontinued in the future.  The current location of the code library is within the NMproject package in the relative path `inst/extdata/CodeLibrary`.  To contribute, either use the pull request functionality of GitHub or send me an email with your change.  Make sure you describe the reason for the change so I can ensure it will suit everyone. 

### + We already have directory of R/NONMEM scripts/templates, how can we also use these?

Append your directory location to the `code_library_path` option.

```{r eval=FALSE}
options(code_library_path = c(getOption("code_library_path"),
                              "path/to/existing/repository"))
```

You'll probably want to set this for all users so see the question above on setting options permanently

### + How do I run this on SGE?

There is a pre-prepared built in `sge_parallel_execute` character object that's part of NMproject.  This uses the grid functionality built into PsN and has been tested to work within the Metworx platform.  Simply type it in the console to see the contents.  Required fields are `parafile`, `cores`.  Ensure these are set for your parent object like so.

```{r eval=FALSE}
m1 <- m1 %>% 
  cmd(sge_parallel_execute) %>%
  parafile("/opt/NONMEM/nm75/run/mpilinux8.pnm") %>%
  cores(8) %>%
  run_nm()
```

Note that child object will inherit the same `cmd` structure, `cores` and `parafile`.

### + How do I run this on other clusters like Slurm, LSF, Torque

The workflow is similar to above where PsN handles the grid submission. You will need to create your own analog character to `sge_parallel_execute` for your respective cluster.  It is recommended to consult PsN documentation to "gridify" your PsN command.  Once you have this, it's just a simple matter of replacing your control file name, run directory, parafile and desired number of cores with the relevant glue field (e.g. `{parafile}`) and then putting it into your parent `cmd()` command to get it running through NMproject.

Feel free to contact me if you need help

### + How do I run NONMEM via a PsN/NONMEM docker container

This requires setting `cmd()` field of the first (parent) nm object and also setting `nm_tran_command()`.

Easiest way to understand this is via an example: The following assumes the docker container has been set as shown in the fabulous `https://github.com/billdenney/Pharmacometrics-Docker` repository:

Set cmd for an `execute` command like so:

```{r eval=FALSE}
m1 %>% 
  cmd("docker run --rm --user=$(id -u):$(id -g) -v $(pwd):/data -w /data humanpredictions/psn execute {ctl_name} -dir={run_dir}")
```

`run_nm()` will then execute NONMEM via the docker container.  All subsequent child objects will inherit the same command structure.  Note the use the glueing object fields `ctl_name` and `run_dir` so child objects can inherit the same command structure to save the command being rewritten for each run)

Set up dockerized NMTRAN checking with:

```{r eval=FALSE}
nm_tran_command("docker run --rm --user=$(id -u):$(id -g) -v $(pwd):/data -w /data humanpredictions/psn /bin/bash -c '/opt/NONMEM/nm_current/tr/NMTRAN.exe < {ctl_name}'")
```

### + My Rstudio Server is on a different linux server to my NONMEM cluster.  How can I set up NMproject to work with this?

You need to ensure your account has passwordless ssh set up.  Then create a `system_nm` option in your `~/.Rprofile` configuration file which appends an ssh statement to the system call e.g. the following will set you up to connect to the host `clustername`:

```{r eval=FALSE}
options(system_nm=function(cmd,...) {
        system(paste0("ssh -q clustername \"cd $(pwd); ",cmd,"\""),...)
})
```

### + I'm working on a windows laptop but want to use my NONMEM cluster for NONMEM jobs.  How can I set up NMproject to work with this?

This is not recommended as it requires R working directory being set to a networked drive.  This is very slow.  If you really want to though consider modifying the `system_nm()` option, as in the above FAQ question, to use `plink` to ssh to the server, change to the relevant working directory and submit a command.  This has not been tested however and results are likely to be disappointing.

### + My organisation has a different control file convention to the runXX.mod convention.  Can I change this?

Set it up with `ctl_path` the field of your object, e.g. to change the convention to `nm.XX.con`

```{r eval=FALSE}
m %>% ctl_path("Models/nm.{run_id}.com")
```

### + How do I submit a generic command directly to the NONMEM server?

```{r eval=FALSE}
system_nm("command_to_run", dir="path/to/dir")
```

if dir is not specified, this will default to the `Models` directory

### + There is functionality in PsN's runrecord, sumo or Pirana that I would like but is not available in NMproject.

NMproject doesn't change PsN's default directory structure, everything in the "Models" directory is as if you lauched the jobs from the command line.  Therefore you can continue to use PsN functions on the command line.  You can also continue using Pirana by pointing it towards your models directory.

If it's something you think really should be part of NMproject, open a github "issue" and ask for the feature.

### + I don't want to use NMproject on my analysis project any more, can I go back to submitting runs on the command line

Yes, NMproject has been built to reduce as much as possible the barrier-to-exit to ensure all usage is a voluntary as possible.  NMproject doesn't change PsN's default directory structure, so you can go back to running PsN via command line or using Pirana etc., at any point.

### + I work for a CRO. My client doesn't have NMproject or doesn't know to use NMproject, how can send the analysis to them.

NMproject doesn't change PsN's default directory structure, and everything will work for them as long as their version of R (and package versions) are compatible.  Your model development log/notebook can still serve as a helpful, human readable process description of your model development steps though.  You just need to explain how `run_id`s correspond to files (e.g. `m1` corresponds to `Models/runm1.mod`) so they are able to track what is happening in the model development log to the files on the disk.

If NMproject will be rerun in a different environment consider using `renv` to ensure package versions remain consistent.

```{r include=FALSE}
## clean up 
getwd()
unlink(copied_folders, recursive = TRUE)
dir(all.files = TRUE)
```

