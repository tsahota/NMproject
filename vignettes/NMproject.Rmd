---
title: "NMproject"
author: "Tarj Sahota"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{NMproject}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Vignette Info

The intent of NMproject is to allow for fully script based NONMEM model development:

* Code management:
  * Code library: NONMEM template scripts to speed creation of NONMEM runs and facilitate adherence to best practices.
  * Standardised, version controlled, directory structure for all NONMEM users.
* Script based model development:
  * Instead of command line interface (CLI) or graphical user interface (GUI) use a script based interface
  * End-to-end R workflows that can be repeated, reused and documented in R markdown
  * Convenience functions for routine NONMEM control file edits
  * Tracked manual edits for everything else to ensure full flexibility
  * Fully customizable r markdown templates for goodness of fits, vpcs, ppcs, bootstraps
  * Vectorized model objects - interact with groups of NONMEM runs as if they were one (no loops needed)
  * Shiny interface for real time run tracking in grid environments

History: NMproject was previously an AstraZeneca project.  It is being reimplemented here as a community version to be compatible with a variety of architectures (standalone NONMEM and a variety of grid submission systems)

```{r setup, include=FALSE}
library(knitr)
library(NMproject)
library(dplyr)
extdata_path <- system.file("extdata", package = "NMproject")

testfilesloc <- file.path(extdata_path, "theopp")
zip_file <- file.path(extdata_path, "theopp.zip")

files_to_unzip <- unzip(zip_file, list = TRUE)

unzip(zip_file)

dir(all.files = TRUE)
extracted_folders <- dir("theopp", full.names = TRUE)

for(extracted_folder in extracted_folders){
  file.copy(extracted_folder, ".", recursive = TRUE)
}

file.rename("cache", ".cache")
dir(all.files = TRUE)

copied_folders <- basename(gsub("cache", ".cache", extracted_folders))

unlink("theopp", recursive = TRUE)

orig_dir <- getwd()

```


## Demo

The easiest way to familiarise your with NMproject is to follow through the demo.

First create a new tidyproject (`FILE` -> `New Project` -> `New Directory` -> `New tidyproject`). You'll see a clean analysis directory with empty subfolders.  Run the following in the new R session:

```{r eval = FALSE}
library(NMproject)
setup_nm_demo()
```

This will population the `Scripts` directory with scripts, and deposit the Theopp dataset into the `SourceData` subdirectory.

The scripts are numbered s01.....Rmd, s02....Rmd etc., and are designed to be read and run in order.  This is the best way to familiarize yourself.  The scripts can also be used as template for your own model development

## NMproject Usage

### Creating new R markdowns

There is nothing mandating the use of R markdown in NMproject.  You can use scripts.  However R markdown documents produce nice sharable model development logs which provide a readable description of what steps were performed and in what order.

It is advisable to always start from a template.  Templates can be accessed in `New Rmarkdown` -> `From Template`.  One example template is `model generic` which can be used to get started with our dataset cleaning and exploratory plotting.

### Data cleaning

Your first script in performing an analysis will probably be a dataset assembly or dataset cleaning.  The intent of the `SourceData` directory is to serve as an entry point of raw or externally produced dataset into your analysis directory.

It is *best practice* to use relative paths in your code where possible and to avoid using `setwd()` commands to change the working directory.  E.g. to read a data set in the `SourceData` directory, use `read.csv("SourceData/data.csv")`.

The `write_derived_data()` function can be used to save the NONMEM ready dataset into the `DerivedData` directory:

```{r eval = FALSE}

d %>% write_derived_data("DerivedData/data.csv")

```

See the demo for how/why to create bootstrapped datasets before beginning model development.

### Model development

`New Rmarkdown` -> `From Template` -> `Model development` will get you started with a model development log file.

It's always best to start with NONMEM template too.  Search the code library and bring into staging area of analysis project using the `code_library` addin:

One of the templates is `Models/ADVAN2.mod`.  Select it, `preview()` it and then `stage()` by clicking the buttons and then closing the app.

<iframe width="560" height="315" src="https://www.youtube.com/embed/nTixZB9tfgk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

The model will be in the staging area: `staging/Models/ADVAN2.mod`, not `Models/ADVAN2.mod`.  Files in the staging area are not to be modified. It's similar to the `SourceData` directory in that sense.  This is to ensure that we can make changes to `ADVAN2.mod` in the code library without breaking old analyses that depended on it.

The other important component to understand is that control files are only written to the `Models` subdirectory just before running nonmem via the `run_nm()` command.  Prior to this, control file information is stored within the object itself.

Let's create our first NONMEM object with `new_nm()`.  This will be a parent run to all other runs and thus requires more set up than other runs. Subsequent child objects will inherits characteristics of the parent.

Three arguments are required to create the parent, a run identifier, `run_id`, a control file it's based on, `based_on`, and a (relative) dataset path `data_path`:

```{r include=FALSE}

ls_code_library("Models/ADVAN2.mod") %>%
  stage()

```


```{r}

m1 <- new_nm(run_id = "m1",
             based_on = "staging/Models/ADVAN2.mod",
             data_path = "DerivedData/data.csv")

m1

```

Display the object by typing `m1` in the console.  Notice that the `run_in` field is point to `Models`.  This can be changed by piping the function of the same name e.g. 

```{r}

m1 <- new_nm(run_id = "m1",
             based_on = "staging/Models/ADVAN2.mod",
             data_path = "DerivedData/data.csv") %>%
  run_in("NONMEM/base_model")

```

will run all models in a subdirectory of a subdirectory `NONMEM/base_model` instead.  Any field can by modified using a similar heuristic.

To extract a field (rather than set a field) use the same function without additional arguments:

```{r}

m1 %>% run_in()

```

For now though, lets stay with the default `run_in` location.  

*NOTE:* the field `ctl_name` refers to the name of the control that will be created.  This will only be created when it the model is run with the `run_nm()` function (described later).  For now, the control file contents reside inside the object.  To view these you can use `show_ctl(m1)`, or `text(m1)`.

A few automatic edits from the staged control file and a compact representation of these changes can be shown by highlighting the above code and selecting the `nm_diff` addin which show what has been changed.

<iframe width="560" height="315" src="https://www.youtube.com/embed/mYUaKrj0-YE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Learning how to read to diffs will be an important skill in NMproject you will pick up over time.  Notice how the `$DATA` has been updated to refer to the new location.

The default `cmd()` field of the object is `execute {ctl_name} -dir={run_dir}`.  The braces are referred to as `glue` fields using the `glue` package.  These refer to field names of the object that will be substituted in.  For completeness on the next step we will explicitly set this to ensure our model development is easy to read.

```{r eval=FALSE}

m1 <- new_nm(run_id = "m1",
             based_on = "staging/Models/ADVAN2.mod",
             data_path = "DerivedData/data.csv") %>%
  cmd("execute {ctl_name} -dir={run_dir}")

```

The final steps to gets the NONMEM model ready is to fill in the remaining blanks in the template.  They are the `$INPUT` and `$THETA`, `$OMEGA`.  For this we will use the `fill_input()` and `init_theta()` and `init_omega()` before piping into our `run_nm()` command.

```{r eval=FALSE}

m1 <- new_nm(run_id = "m1",
             based_on = "staging/Models/ADVAN2.mod",
             data_path = "DerivedData/data.csv") %>%
  cmd("execute {ctl_name} -dir={run_dir}") %>%
  fill_input() %>%
  init_theta(init = c(-2, 0.5, 1)) %>%
  init_sigma(init = c(0.1, 0.1)) %>%
  run_nm()

```

### Fast NMTRAN checks

In a cluster environment it is especially important to be able to diagnose data and control file issues before sending jobs off to the grid.  Otherwise, you often need to wait minutes for errors that can be spotted immediately.  It's therefore highly recommended to make use of the `nm_tran` addin.  This will only run NMTRAN checks to find control file syntax errors and dataset errors.  It will not run nonmem itself.  To use this addin, highlight the above code and select the `nm_tran` addin near the top of the RStudio GUI.

### Manual edits

Often there will not be the functions to do the control file manipulation you want.  Although it is preferable to stick to automatic control file manipulation functions, you can do full tracked `manual_edit`s via the addin menu.  Again, just highlight the code, click the addin and follow the instructions:

<iframe width="560" height="315" src="https://www.youtube.com/embed/HyCHeAAiwy4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### Shiny run monitor

To view all runs in the workspace and track progress:

```{r eval=FALSE}
shiny_nm()
```

To do basic goodness of fit open the post processing Rmarkdown template, follow instructions, customise your template save it (e.g. as `Scripts/basic_gof.Rmd`) and run in your log script with

```{r eval=FALSE}
m1 <- m1 %>% nm_render("Scripts/basic_gof.Rmd")
```

It will create the output in `Results` (or results_dir(m1))

If we already have a parent run `m1` using `ADVAN2 TRANS1`, we can create a `child()` run that uses `TRANS2` using `subroutine()`:

```{r include=FALSE}

m1 <- dir(".cache", full.names = TRUE) %>%
  subset(grepl("m1$",.)) %>%
  {as_nm_list(readRDS(.)$object)}

m2 <- dir(".cache", full.names = TRUE) %>%
  subset(grepl("m2$",.)) %>%
  {as_nm_list(readRDS(.)$object)}

m2WT <- dir(".cache", full.names = TRUE) %>%
  subset(grepl("m2WT$",.)) %>%
  {as_nm_list(readRDS(.)$object)}

m1orig <- m1

## need to skip overwrite because new user will mean directory gets overwriten
overwrite_behaviour("skip") 

```

```{r eval=FALSE}

m2 <- m1 %>% child() %>%
  subroutine(trans = 2) %>% 
  run_nm()

```

View exactly what's been changed by highlighting the above code in RStudio and clicking the `nm_diff` addin to see what's been changed before running.  Here, changes will be in the `$SUB`, `$PK` and `$THETA`.

To add a covariate using PsN coding conventions use `add_cov()`:

```{r eval=FALSE}

m2WT <- m2 %>% child(run_id = "m2WT") %>%
  add_cov(param = "CL", cov = "WT", state = "linear") %>%
  run_nm()

```

Apply a fully editable goodness of fit R markdown template to both runs `m2` and `m3`:

```{r eval=FALSE}

c(m2, m3) %>% nm_render("Scripts/basic_gof.Rmd")
## will produce two run specific html reports in the "Results" directory for run evaluation

```

`c(m1, m3)` is a vector object of 2 NONMEM runs.  It can be embedded into data.frames/tibbles.  

Evaluate and compare runs on the fly with the following commands

```{r}
rr(c(m2, m2WT))
plot_iter(m2, skip = 30) ## skip first 30 interations 
covariance_plot(m2)
```

However these are better placed inside templates to enable rapid and consistent re-use.

Create a simulation run using `update_parameters()` and `convert_to_simulation()`.  Apply vpc and ppc diagnostics
  
```{r eval=FALSE}
m2s <- m2 %>% child(run_id = "m2s") %>%
              update_parameter(m2) %>%
              convert_to_simulation(subpr = 50) %>%
              run_nm()

m2s %>% nm_render("Scripts/basic_vpc.Rmd")
m2s %>% nm_render("Scripts/basic_ppc.Rmd")

```

Don't forget to comment your code with your decision making.

### Clean up runs

```{r eval=FALSE}

m1 %>% ls_tempfiles() ## will list all temporary files associate with run m1

## remove all temporary files associated with m1
m1 %>% ls_tempfiles() %>% unlink()

## Not specifying an argument will list temporary files of all runs
ls_tempfiles()

```

### Running NONMEM in parallel

Two fields `parafile` and `cores` when combined with a suitable `cmd` can enable runs to be executed in parallel using PsN.

```{r eval=FALSE}
m1 <- new_nm(run_id = "m1",
             based_on = "staging/Models/ADVAN2.mod",
             data_path = "DerivedData/data.csv") %>%
  cmd("execute {ctl_name} -parafile={parafile} -dir={run_dir} -nodes={cores}") %>%
  parafile("/opt/NONMEM/nm750/run/mpilinux8.pnm") %>%
  cores(4) %>%
  run_nm()

```

```{r include = FALSE}
m1 <- m1 %>%
  cmd("execute {ctl_name} -parafile={parafile} -dir={run_dir} -nodes={cores}") %>%
  parafile("/opt/NONMEM/nm750/run/mpilinux8.pnm") %>%
  cores(4)
```

*ADVANCED* You can make use of the vectorized objects to run multiple levels of parallelisation to invest which is the best for your particular model.  Here is sample code for that uses `m1` as a parent

```{r}
dc <- tibble(cores = c(1, 3, 10, 30)) %>%
  mutate(m = m1 %>% 
           child(run_id = cores) %>%
           cores(cores) %>%
           run_in("Models/m1_coretest"))

dc

dc$m %>% cmd()

# dc$m %>% run_nm()  ## command to run

```


### Bootstraps

The package `rsample` can be used to create bootstrap datasets in your initial data manipulation scripts.  The following is an example bootstrap dataset being prepared with stratification on `SEX` and bodyweight `WTC` categorised in two categories  

```{r eval = FALSE}

d <- d %>%
  mutate(WT_C = cut(WT, breaks = 2, labels = FALSE),
         STRATA = paste(SEX, WT_C, sep = "_"))

d_id <- d %>% distinct(ID, STRATA)

set.seed(123)

## create large set of resamples (to enable simulation to grow without ruining seed)
bootsplits <- rsample::bootstraps(d_id, 100, strata = "STRATA")

dir.create("DerivedData", showWarnings = FALSE)
bootsplits %>% saveRDS("DerivedData/bootsplit_data.csv.RData")

```

In a model development script, the following, performs a 100 sample bootstrap of model `m1`

```{r eval=FALSE}

m1_boot <- m1 %>% make_boot_datasets(samples = 100, overwrite = TRUE)

m1_boot$m <- m1_boot$m %>% run_nm()

m1_boot$m %>% nm_list_render("Scripts/basic_boot.Rmd")

```

Results can be viewed in `Results/basic_boot.m1.html`.

## FAQ

### + After having closed my session how to I recreate my workspace

Just re-run all the code again.

If nothing has changed with regards to datasets and control files run_nm() will retrieve cached results without running nonmem.


### + I see the PMX code library has been updated, how can I update my local version of it?

The PMX code library is version controlled with `git`.  Ensure you have git installed, navigate to the local PMX code library repository and type `git pull`.  Git will complain if you have made changes to your local directory since downloading it.  If you have, you need to `commit` those changes before using `git pull`.  See git help for more information on how to do this.  Consider splitting your code library into multiple repositories for easier management (e.g. one public, one for your organisation, one for you)

### + We already have directory of R/NONMEM scripts/templates, how can we also use these?

Append your directory location to the `code_library_path` option.  To do this add the following command to your `~/.Rprofile` (or `$R_HOME/etc/Rprofile.site`) configuration file:

```{r eval=FALSE}
options(code_library_path = c("/path/to/PMXcodelibrary/","path/to/existing/repository"))
```

### + How can I contribute to the PMX code library?

Log in to github (create an account if necessary).  Fork the repository `tsahota/PMXcodelibrary`. Make your change.  Create a pull request detailing your change.

### + How do I run this on SGE?

There is a pre-prepared built in `sge_parallel_execute` character object that's part of NMproject.  This uses the grid functionality built into PsN and has been tested to work within the Metworx platform.  Simply type it in the console to see the contents.  Required fields are `parafile`, `cores`.  Ensure these are set for your parent object like so.

```{r eval=FALSE}
m1 <- m1 %>% 
  cmd(sge_parallel_execute) %>%
  parafile("/opt/NONMEM/nm750/run/mpilinux8.pnm") %>%
  cores(8) %>%
  run_nm()
```

Note that child object will inherit the same `cmd` structure, `cores` and `parafile`.

### + How do I run this on other clusters like Slurm, LSF, Torque

The workflow is similar to above where PsN handles the grid submission. You will need to create your own analog character to `sge_parallel_execute` for your respective cluster.  It is recommended to consult PsN documentation to "gridify" your PsN command.  Once you have this, it's just a simple matter of replacing your control file name, run directory, parafile and desired number of cores with the relevant glue field (e.g. `{parafile}`) and then putting it into your parent `cmd()` command to get it running through NMproject.

Feel free to contact me if you need help

### + How do I run NONMEM via a PsN/NONMEM docker container

This requires setting `cmd()` field of the first (parent) nm object and also setting `nm_tran_command()` once in your script.

Easiest way to understand this is via an example: The following assumes the docker container has been set as shown in the fabulous `https://github.com/billdenney/Pharmacometrics-Docker` repository:

Set cmd for an `execute` command like so:

```{r eval=FALSE}
m1 %>% 
  cmd("docker run --rm --user=$(id -u):$(id -g) -v $(pwd):/data -w /data humanpredictions/psn execute {ctl_name} -dir={run_dir}")
```

`run_nm()` will then execute NONMEM via the docker container.  All subsequent child objects will inherit the same command structure.  Note the use the glueing object fields `ctl_name` and `run_dir` so child objects can inherit the same command structure to save the command being rewritten for each run)

Set up dockerized NMTRAN checking with:

```{r eval=FALSE}
nm_tran_command("docker run --rm --user=$(id -u):$(id -g) -v $(pwd):/data -w /data humanpredictions/psn /bin/bash -c '/opt/NONMEM/nm_current/tr/NMTRAN.exe < {ctl_name}'")
```


### + My Rstudio Server is on a different linux server to my NONMEM cluster.  How can I set up NMproject to work with this?

You need to ensure your account has passwordless ssh set up.  Then create a `system_nm()` option in your `~/.Rprofile` configuration file which appends an ssh statement to the system call e.g. the following will set you up to connect to the host `clustername`:

```{r eval=FALSE}
options(system_nm=function(cmd,...) {
        system(paste0("ssh -q clustername \"cd $(pwd); ",cmd,"\""),...)
})
```

### + I'm working on a windows laptop but want to use my NONMEM cluster for NONMEM jobs.  How can I set up NMproject to work with this?

This is not recommended as it requires R working directory being set to a networked drive.  This is very slow.  If you really want to though consider modifying the `system_nm()` option, as in the above FAQ question, to use `plink` to ssh to the server, change to the relevant working directory and submit a command.  This has not been tested however and results are likely to be disappointing.

### + My organisation has a different control file convention to the runXX.mod convention.  Can I change this?

Yes, you can specify the convention with the ctl_path() on your parent object e.g. to change the convention to nm.XX.con

```{r eval=FALSE}
m %>% ctl_path("Models/nm.{run_id}.com")
```

### + How do I submit a command directly to the NONMEM server?

```{r eval=FALSE}
system_nm("command_to_run", dir="path/to/dir")
```

### + There is functionality in PsN's runrecord, sumo or Pirana that I would like but is not currently available in NMproject.

NMproject doesn't change PsN's default directory structure, everything in the "Models" directory is as if you lauched the jobs from the command line.  Therefore you can continue to use PsN functions on the command line.  You can also continue using Pirana by pointing it towards your models directory.

If it's something you think really should be part of NMproject, open a github "issue" and ask for the feature.

### + I don't want to use NMproject on my analysis project any more, can I go back to submitting runs on the command line

Yes, NMproject doesn't change PsN's default directory structure, so you can go back to running PsN via command line.  If there a bug or a feature you think really should be part of NMproject, consider opening a github "issue" and asking.

### + I work for a CRO. My client doesn't have NMproject, how can send the analysis to them.

NMproject doesn't change PsN's default directory structure, and everything will work for them as long as their version of R (and package versions) are compatible.  It is recommended to run Renvironment_info() as a last step before sending the analysis directory so they can see the package versions you used.

Obviously they will not be able to run code that is dependent on NMproject unless they install it.  But your model development script can still serve as a helpful, human readable process description of your model development steps.


```{r include=FALSE}
## clean up 
getwd()
unlink(copied_folders, recursive = TRUE)
dir(all.files = TRUE)
```

